#define ENTRY(name)      \
        .globl name  ;   \
        .align 4 ;       \
        name:

#define ENDPROC(name)       \
        .type name  2 ;     \
        .size name, .-name

/*
 * Branch according to exception level
 */
.macro  switch_el, xreg, el3_label, el2_label, el1_label
    mrs \xreg, CurrentEL
    cmp \xreg, 0xc
    b.eq    \el3_label
    cmp \xreg, 0x8
    b.eq    \el2_label
    cmp \xreg, 0x4
    b.eq    \el1_label
.endm


ENTRY(__asm_flush_dcache_level)
    lsl x12, x0, #1
    msr csselr_el1, x12     /* select cache level */
    isb             /* sync change of cssidr_el1 */
    mrs x6, ccsidr_el1      /* read the new cssidr_el1 */
    and x2, x6, #7      /* x2 <- log2(cache line size)-4 */
    add x2, x2, #4      /* x2 <- log2(cache line size) */
    mov x3, #0x3ff
    and x3, x3, x6, lsr #3  /* x3 <- max number of #ways */
    clz w5, w3          /* bit position of #ways */
    mov x4, #0x7fff
    and x4, x4, x6, lsr #13 /* x4 <- max number of #sets */
    /* x12 <- cache level << 1 */
    /* x2 <- line length offset */
    /* x3 <- number of cache ways - 1 */
    /* x4 <- number of cache sets - 1 */
    /* x5 <- bit position of #ways */

loop_set:
    mov x6, x3          /* x6 <- working copy of #ways */
loop_way:
    lsl x7, x6, x5
    orr x9, x12, x7     /* map way and level to cisw value */
    lsl x7, x4, x2
    orr x9, x9, x7      /* map set number to cisw value */
    tbz w1, #0, 1f
    dc  isw, x9
    b   2f
1:  dc  cisw, x9        /* clean & invalidate by set/way */
2:  subs    x6, x6, #1      /* decrement the way */
    b.ge    loop_way
    subs    x4, x4, #1      /* decrement the set */
    b.ge    loop_set

    ret
ENDPROC(__asm_flush_dcache_level)

/*
 * void __asm_flush_dcache_all(int invalidate_only)
 *
 * x0: 0 flush & invalidate, 1 invalidate only
 *
 * clean and invalidate all data cache by SET/WAY.
 */
ENTRY(__flush_cache_all)
    mov x1, x0
    dsb sy
    mrs x10, clidr_el1      /* read clidr_el1 */
    lsr x11, x10, #24
    and x11, x11, #0x7      /* x11 <- loc */
    cbz x11, finished       /* if loc is 0, exit */
    mov x15, x30
    mov x0, #0          /* start flush at cache level 0 */
    /* x0  <- cache level */
    /* x10 <- clidr_el1 */
    /* x11 <- loc */
    /* x15 <- return address */

loop_level:
    lsl x12, x0, #1
    add x12, x12, x0        /* x0 <- tripled cache level */
    lsr x12, x10, x12
    and x12, x12, #7        /* x12 <- cache type */
    cmp x12, #2
    b.lt    skip            /* skip if no cache or icache */
    bl  __asm_flush_dcache_level    /* x1 = 0 flush, 1 invalidate */
skip:
    add x0, x0, #1      /* increment cache level */
    cmp x11, x0
    b.gt    loop_level

    mov x0, #0
    msr csselr_el1, x0      /* restore csselr_el1 */
    dsb sy
    isb
    mov x30, x15

finished:
    ret
ENDPROC(__flush_cache_all)


/*
 * void __asm_invalidate_tlb_all(void)
 *
 * invalidate all tlb entries.
 */
ENTRY(__invalidate_tlb_all)
    switch_el x9, 3f, 2f, 1f
3:  tlbi    alle3
    dsb sy
    isb
    b   0f
2:  tlbi    alle2
    dsb sy
    isb
    b   0f
1:  tlbi    vmalle1
    dsb sy
    isb
0:
    ret
ENDPROC(__invalidate_tlb_all)

